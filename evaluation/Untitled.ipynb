{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the over fit score is:', 0.98947368421052628)\n",
      "('the score after train_test_split:', 0.72413793103448276)\n",
      "('How many POIs are in the test set for your POI identifier?', 4)\n",
      "('How many people total are in your test set?', 29)\n",
      "('accuracy in prediction', 0.8620689655172413)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Starter code for the validation mini-project.\n",
    "    The first step toward building your POI identifier!\n",
    "\n",
    "    Start by loading/formatting the data\n",
    "\n",
    "    After that, it's not our code anymore--it's yours!\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "data_dict = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### first element is our labels, any added elements are predictor\n",
    "### features. Keep this the same for the mini-project, but you'll\n",
    "### have a different feature list when you do the final project.\n",
    "features_list = [\"poi\", \"salary\"]\n",
    "\n",
    "data = featureFormat(data_dict, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "\n",
    "### it's all yours from here forward!  \n",
    "\n",
    "###Quiz: Your First (Overfit) POI Identifier:  Create a decision tree classifier (just use the default parameters), train it on all the data (you will fix this in the next part!), and print out the accuracy. THIS IS AN OVERFIT TREE, DO NOT TRUST THIS NUMBER! Nonetheless, what’s the accuracy?\n",
    "from sklearn import tree\n",
    "clf1 = tree.DecisionTreeClassifier()\n",
    "clf1 = clf1.fit(features, labels)\n",
    "\n",
    "print('the over fit score is:',clf1.score(features, labels))\n",
    "\n",
    "\"\"\"Now you’ll add in training and testing, \n",
    "so that you get a trustworthy accuracy number. \n",
    "Use the train_test_split validation available in sklearn.cross_validation; \n",
    "hold out 30% of the data for testing and set the random_state parameter to 42 \n",
    "(random_state controls which points go into the training set and which are used \n",
    "for testing; setting it to 42 means we know exactly which events are in which set, \n",
    "and can check the results you get). What’s your updated accuracy?\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "features_train, features_test, labels_train, labels_test= train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\"\"\"clf = svm.SVC(kernel='linear', C=1).fit(features_train, labels_train)\"\"\"\n",
    "\n",
    "clf2 = tree.DecisionTreeClassifier()\n",
    "clf2 = clf2.fit(features_train,labels_train)\n",
    "print('the score after train_test_split:',clf2.score(features_test, labels_test))\n",
    "\n",
    "# How many POIs are in the test set for your POI identifier?\n",
    "pred = clf2.predict(features_test)\n",
    "sum(pred)\n",
    "print (\"How many POIs are in the test set for your POI identifier?\",len([e for e in labels_test if e == 1.0]))\n",
    "\n",
    "\n",
    "# How many people total are in your test set?\n",
    "print(\"How many people total are in your test set?\",len(pred))\n",
    "\n",
    "\n",
    "# If your identifier predicted 0. (not POI) for everyone in the test set, what would its accuracy be?\n",
    "print (\"accuracy in prediction\", 1.0-4.0/29)\n",
    "\n",
    "\n",
    "#Look at the predictions of your model and compare them to the true test labels. Do you get any true positives? (In this case, we define a true positive as a case where both the actual label and the predicted label are 1)\n",
    "#print pred\n",
    "#print (labels_test)\n",
    "\n",
    "#Use the precision_score, What’s the precision?\n",
    "from sklearn.metrics import *\n",
    "precision_score(labels_test, pred)\n",
    "\n",
    "# What’s the recall? \n",
    "recall_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In the final project you’ll work on optimizing your POI identifier, using many of the tools learned in this course. Hopefully one result will be that your precision and/or recall will go up, but then you’ll have to be able to interpret them. \n",
    "\n",
    "Here are some made-up predictions and true labels for a hypothetical test set; fill in the following boxes to practice identifying true positives, false positives, true negatives, and false negatives. Let’s use the convention that “1” signifies a positive result, and “0” a negative. \n",
    "\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1] \n",
    "true labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "How many true positives are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666666666667\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1] \n",
    "true_labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "\n",
    "\n",
    "tp=6\n",
    "tn=9\n",
    "fp=3\n",
    "fn=2\n",
    "\n",
    "print precision_score(true_labels,predictions)\n",
    "print recall_score(true_labels,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
